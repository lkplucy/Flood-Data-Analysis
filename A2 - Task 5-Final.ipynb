{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fc7f33-94fe-4ab8-94ff-27347c335568",
   "metadata": {},
   "source": [
    "# Prepare SQuAD_tiny Dataset for Assignment 2\n",
    "\n",
    "This code prepare SQuAD_tiny from the SQuAD dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc501e-66e7-4dd8-9ede-7dd516c57230",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe47fc-6d7a-4fcb-b07f-7435895e88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c25606-856c-4e9b-bbb3-86d0abdd798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11000feb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9a46e-5f00-41af-9148-a6eefb6d08ed",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0437040-ad9b-4cfe-9801-d1a801a7102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bd720f-173b-47ea-9c85-abe55b0ae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "#train_dataset = dataset[\"train\"].select(range(10000))\n",
    "#val_dataset = dataset[\"validation\"].select(range(1000))\n",
    "#test_dataset = dataset[\"validation\"].select(range(1000, 2000))  # No official SQuAD test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9afc575-7f55-44f7-8cc5-d30bfdb6e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(1000))\n",
    "val_dataset = dataset[\"validation\"].select(range(100))\n",
    "test_dataset = dataset[\"validation\"].select(range(100, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98eb33ba-6f3f-4b5e-9cad-3079129e2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "SQuAD_tiny = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0f0e1d-988a-4ddd-ba32-ced7e6d1dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2be2d1-6826-485f-a531-e2b78711e25a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f33ad7-59db-421c-888c-7fe7da8cc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_input_length = 512\n",
    "Max_output_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3e2652a-32ac-4754-8bb5-3a8260021062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "#def preprocess(example):\n",
    "#    input_text = f\"question: {example['question']} context: {example['context']}\"\n",
    "#    target_text = example[\"answers\"][\"text\"][0]\n",
    "#    input_enc = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=Max_input_length)\n",
    "#    target_enc = tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=Max_output_length)\n",
    "#    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "#    return input_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "58590961-a7ce-4e6c-a21e-bcfe58274e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging as transformers_logging\n",
    "\n",
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question} context: {context}\"\n",
    "\n",
    "def extract_example_parts(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "    question_with_context = encode_question_and_context(question,context)\n",
    "    return (question_with_context, question, answer)\n",
    "\n",
    "def preprocess(example):\n",
    "    question_with_context, question, answer = extract_example_parts(example)\n",
    "\n",
    "    old_level = transformers_logging.get_verbosity()\n",
    "    transformers_logging.set_verbosity_error()\n",
    "\n",
    "    input_enc = tokenizer(question_with_context, question, padding = \"max_length\",\n",
    "                             truncation = True, max_length = Max_input_length)\n",
    "    target_enc = tokenizer(answer, padding = \"max_length\", truncation = True,\n",
    "                           max_length = Max_output_length)\n",
    "    transformers_logging.set_verbosity(old_level)\n",
    "\n",
    "    input_enc[\"labels\"] = np.array(target_enc[\"input_ids\"])\n",
    "\n",
    "    return input_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88504d94-9c04-4af6-965e-742418624148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "train_enc = train_dataset.map(preprocess, batched=False)\n",
    "val_enc = val_dataset.map(preprocess, batched=False)\n",
    "test_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "058e075b-721c-47f1-9429-c3b7d320aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71900ed4-a318-4d9a-9ccb-36c93a3183f7",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f0bbda5-04be-4603-be6e-887c9fafe9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb0261-4843-4679-8ba7-106988ab1b6e",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a47fac1-457f-4289-b471-5812c15f5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_enc.set_format(type = \"torch\", columns = columns)\n",
    "val_enc.set_format(type = \"torch\", columns = columns)\n",
    "test_enc.set_format(type = \"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8817736-3654-407c-867e-e5482b156a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/375 00:09 < 29:19, 0.21 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      5\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer)\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2241\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2242\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2243\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2244\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2245\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3791\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3789\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2473\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2473\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42203950-87b6-4114-b0a0-5d7a3d2116c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 3))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32425948-3db5-480d-8caf-a89f8e81b1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 23:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.173281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.133859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.124995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.16634305795033774, metrics={'train_runtime': 1423.6643, 'train_samples_per_second': 2.107, 'train_steps_per_second': 0.263, 'total_flos': 406025404416000.0, 'train_loss': 0.16634305795033774, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ec2c0f0-5628-4a20-86d8-6f83a708116d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 17:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.015123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.014601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.025663533329963684, metrics={'train_runtime': 1041.048, 'train_samples_per_second': 1.921, 'train_steps_per_second': 0.24, 'total_flos': 270683602944000.0, 'train_loss': 0.025663533329963684, 'epoch': 2.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 2,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 3e-4,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402157c-4fd4-40f6-b4ee-5b39552de94d",
   "metadata": {},
   "source": [
    "## Finetune batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e428f6e3-9c3d-4fcb-b854-6633190b502a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 28:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.106525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.100698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.099714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.13934119983955665, metrics={'train_runtime': 1704.127, 'train_samples_per_second': 1.76, 'train_steps_per_second': 0.111, 'total_flos': 406025404416000.0, 'train_loss': 0.13934119983955665, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "    )\n",
    "\n",
    "model.train()\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72cac368-dd46-474c-8ad3-d81a3aa72228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 58:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.195706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.169136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>0.158253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.3546682445208232, metrics={'train_runtime': 3578.3697, 'train_samples_per_second': 0.838, 'train_steps_per_second': 0.21, 'total_flos': 406025404416000.0, 'train_loss': 0.3546682445208232, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "    )\n",
    "model.train()\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cd80f-d44c-43ca-91c3-08175b278b80",
   "metadata": {},
   "source": [
    "## Weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f48783c5-9dc5-467c-9e53-f3f0172f0109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='693' max='693' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [693/693 37:58, Epoch 11/11]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.067105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.066562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.064834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.063535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.061008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.058313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.056530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.055890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.053894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.053453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.053310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Loss: 0.048\n",
      "Testing Set Loss: 0.054\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.1,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0e109b66-42d8-4bc7-8d9e-6a21ead0ac1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 08:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.051315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.049013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.048228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Loss: 0.046\n",
      "Testing Set Loss: 0.049\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.001,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2eac2-888f-4cc7-8df7-d11e1a8b5b81",
   "metadata": {},
   "source": [
    "## Early Stopping to find the optimal epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7f1757-1056-4afc-bd4b-421e8db8be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 40:05 < 17:13, 0.37 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.143823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.132937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.127384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.119286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.109280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.100075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.087492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.079679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.071966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.066151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.065277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.065485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.066745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.067432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=882, training_loss=0.12902785955913484, metrics={'train_runtime': 2409.2144, 'train_samples_per_second': 8.301, 'train_steps_per_second': 0.523, 'total_flos': 1894785220608000.0, 'train_loss': 0.12902785955913484, 'epoch': 14.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 20,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "\n",
    "    metric_for_best_model = \"loss\",\n",
    "    load_best_model_at_end = True\n",
    "    )\n",
    "\n",
    "model.train()\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e9f654b-9bb0-4ded-8ed6-e1e3b81bbdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.027137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.024793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.0021449310518801214, metrics={'train_runtime': 539.6679, 'train_samples_per_second': 3.706, 'train_steps_per_second': 0.463, 'total_flos': 270683602944000.0, 'train_loss': 0.0021449310518801214, 'epoch': 2.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af908e-6a12-470a-9809-b5d9d6a69273",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e205f24-ef74-4e02-abb9-1dc027543878",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658b7b8-7025-488f-b5a4-a4bfe1c0b86a",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a81a3f0-8079-404b-a4d4-aae1cd324801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Loss: 0.076\n",
      "Testing Set Loss: 0.069\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e166f2a6-b347-40f3-8580-97cebf235dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "\n",
    "def generate_response(tokenizer, model, question):\n",
    "    tokenized = tokenizer(question, return_tensors = \"pt\", padding = True, truncation = True,\n",
    "                          max_length = Max_output_length).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def generate_answers(tokenizer, model, dataset, use_context = True, limit = None):\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset:\n",
    "        question_with_context, question, answer = extract_example_parts(example)\n",
    "\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "\n",
    "        questions.append(question)\n",
    "        \n",
    "        references.append(answer)\n",
    "\n",
    "    outputs = []\n",
    "    for examples in batched(inputs, 128):\n",
    "        responses = generate_response(tokenizer, model, list(examples))\n",
    "\n",
    "        outputs.extend(responses)\n",
    "\n",
    "    #assert (len(outputs) == len(references))\n",
    "    return outputs, references, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1d4ffd43-e47b-45a4-bb01-0cfd5ba94d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(tokenizer, model, test_dataset, use_context = True, limit = 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(tokenizer, model, test_dataset, use_context = False, limit = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bf4d0035-57bd-4d86-9573-3092c952b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. With context\n",
      "\n",
      "Question 1 : Who were special guests for the Super Bowl halftime show?\n",
      "Generated answer: Beyoncé and Bruno Mars\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n",
      "Question 2 : Which Super Bowl halftime show did Beyoncé headline?\n",
      "Generated answer: Super Bowl XLVIII\n",
      "Reference answer: Super Bowl XLVII\n",
      "\n",
      "Question 3 : What was the cost for a half minute ad?\n",
      "Generated answer: $5 million\n",
      "Reference answer: $5 million\n",
      "\n",
      "Question 4 : Who lead the Super Bowl 50 halftime performance?\n",
      "Generated answer: Beyoncé and Bruno Mars\n",
      "Reference answer: Coldplay\n",
      "\n",
      "Question 5 : What other two famous performers were part of the Super Bowl 50 halftime?\n",
      "Generated answer: Beyoncé and Bruno Mars\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n",
      "\n",
      "b. Without context\n",
      "\n",
      "Question 1 : Who were special guests for the Super Bowl halftime show?\n",
      "Generated answer: Wer waren besondere guests für die Super Bowl halftime Show?\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n",
      "Question 2 : Which Super Bowl halftime show did Beyoncé headline?\n",
      "Generated answer: Welche Super Bowl halftime show hat Beyoncé headline headline?\n",
      "Reference answer: Super Bowl XLVII\n",
      "\n",
      "Question 3 : What was the cost for a half minute ad?\n",
      "Generated answer: Was kostete für eine Anzeige in einer halbminütigen Minute?\n",
      "Reference answer: $5 million\n",
      "\n",
      "Question 4 : Who lead the Super Bowl 50 halftime performance?\n",
      "Generated answer: Wer führt die Super Bowl 50 halftime performance?\n",
      "Reference answer: Coldplay\n",
      "\n",
      "Question 5 : What other two famous performers were part of the Super Bowl 50 halftime?\n",
      "Generated answer: Welche anderen zwei berühmte performers waren Teil der halftime der Super Bowl 50?\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question\", i+1,\":\", question)\n",
    "    print(\"Generated answer:\", answer) \n",
    "    print(\"Reference answer:\", reference)\n",
    "\n",
    "print(\"a. With context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i], refs_ctx[i])\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"b. Without context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i], answers_noctx[i], refs_noctx[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aba6ba-c428-4e08-8ba2-090d2964a47c",
   "metadata": {},
   "source": [
    "## Model Evaluation using ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f2a24a84-ddb1-4c93-aa42-bd68674f8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer = True)\n",
    "\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e43d638b-3207-4535-b70c-da79ac0e9f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE with context: {'rouge1_precision': 0.7720833333333332, 'rouge1_recall': 0.7713333333333334, 'rouge1_fmeasure': 0.7644102564102565, 'rouge2_precision': 0.4442857142857143, 'rouge2_recall': 0.45631578947368423, 'rouge2_fmeasure': 0.4446863799283154, 'rougeL_precision': 0.7720833333333332, 'rougeL_recall': 0.7713333333333334, 'rougeL_fmeasure': 0.7644102564102565}\n",
      "\n",
      "ROUGE without context: {'rouge1_precision': 0.017189255189255192, 'rouge1_recall': 0.06333333333333334, 'rouge1_fmeasure': 0.02669069819069819, 'rouge2_precision': 0.007687802393684746, 'rouge2_recall': 0.04, 'rouge2_fmeasure': 0.01274188676820256, 'rougeL_precision': 0.017189255189255192, 'rougeL_recall': 0.06333333333333334, 'rougeL_fmeasure': 0.02669069819069819}\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fec82b-b35a-40c9-8350-48deb0e7231c",
   "metadata": {},
   "source": [
    "## Task 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9199002b-3fb9-4bae-8dc4-bea82bfeff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "\n",
    "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_name)\n",
    "model_2 = AutoModelWithLMHead.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5f9301f7-1d7a-41ea-921d-d1ce6a5df943",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_2, refs_2, questions_2 = generate_answers(\n",
    "    tokenizer_2, model_2, test_dataset, True, 100)\n",
    "answers_n2, refs_n2, questions_n2 = generate_answers(\n",
    "    tokenizer_2, model_2, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b3e868cf-48f5-489a-8daf-6e2069607b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. With context\n",
      "\n",
      "Question 1 : Who were special guests for the Super Bowl halftime show?\n",
      "Generated answer: Beyonce and Bruno Mars\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n",
      "Question 2 : Which Super Bowl halftime show did Beyoncé headline?\n",
      "Generated answer: Super Bowl 50\n",
      "Reference answer: Super Bowl XLVII\n",
      "\n",
      "Question 3 : What was the cost for a half minute ad?\n",
      "Generated answer: $5 million\n",
      "Reference answer: $5 million\n",
      "\n",
      "Question 4 : Who lead the Super Bowl 50 halftime performance?\n",
      "Generated answer: Coldplay\n",
      "Reference answer: Coldplay\n",
      "\n",
      "Question 5 : What other two famous performers were part of the Super Bowl 50 halftime?\n",
      "Generated answer: Beyonce and Bruno Mars\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n",
      "\n",
      "b. Without context\n",
      "\n",
      "Question 1 : Who were special guests for the Super Bowl halftime show?\n",
      "Generated answer: AJ, Nick, and Sean Conner\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n",
      "Question 2 : Which Super Bowl halftime show did Beyoncé headline?\n",
      "Generated answer: Super Bowl Halftime show\n",
      "Reference answer: Super Bowl XLVII\n",
      "\n",
      "Question 3 : What was the cost for a half minute ad?\n",
      "Generated answer: $10.00\n",
      "Reference answer: $5 million\n",
      "\n",
      "Question 4 : Who lead the Super Bowl 50 halftime performance?\n",
      "Generated answer: Kevin Keegan\n",
      "Reference answer: Coldplay\n",
      "\n",
      "Question 5 : What other two famous performers were part of the Super Bowl 50 halftime?\n",
      "Generated answer: Joaquim Doe and Chuckie Lee\n",
      "Reference answer: Beyoncé and Bruno Mars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"a. With context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_2[i], answers_2[i], refs_2[i])\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"b. Without context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_n2[i], answers_n2[i], refs_n2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7b41e31f-117c-4870-a5eb-b024c099851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE with context: {'rouge1_precision': 0.7459444444444445, 'rouge1_recall': 0.7480000000000001, 'rouge1_fmeasure': 0.7368809523809523, 'rouge2_precision': 0.41609689330277566, 'rouge2_recall': 0.4324561403508772, 'rouge2_fmeasure': 0.4154628879892038, 'rougeL_precision': 0.7459444444444445, 'rougeL_recall': 0.7480000000000001, 'rougeL_fmeasure': 0.7368809523809523}\n",
      "\n",
      "ROUGE without context: {'rouge1_precision': 0.11548412698412697, 'rouge1_recall': 0.098, 'rouge1_fmeasure': 0.09732539682539683, 'rouge2_precision': 0.06124999999999999, 'rouge2_recall': 0.04833333333333333, 'rouge2_fmeasure': 0.04966666666666668, 'rougeL_precision': 0.11298412698412698, 'rougeL_recall': 0.0975, 'rougeL_fmeasure': 0.09649206349206349}\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_2, refs_2))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_n2, refs_n2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
