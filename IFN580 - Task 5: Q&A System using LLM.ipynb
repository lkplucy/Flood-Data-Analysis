{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fc7f33-94fe-4ab8-94ff-27347c335568",
   "metadata": {},
   "source": [
    "# Prepare SQuAD_tiny Dataset for Assignment 2\n",
    "\n",
    "This code prepare SQuAD_tiny from the SQuAD dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc501e-66e7-4dd8-9ede-7dd516c57230",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe47fc-6d7a-4fcb-b07f-7435895e88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c25606-856c-4e9b-bbb3-86d0abdd798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10850beb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9a46e-5f00-41af-9148-a6eefb6d08ed",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0437040-ad9b-4cfe-9801-d1a801a7102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42bd720f-173b-47ea-9c85-abe55b0ae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000))\n",
    "val_dataset = dataset[\"validation\"].select(range(1000))\n",
    "test_dataset = dataset[\"validation\"].select(range(1000, 2000))  # No official SQuAD test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0f0e1d-988a-4ddd-ba32-ced7e6d1dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f33ad7-59db-421c-888c-7fe7da8cc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_input_length = 512\n",
    "Max_output_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2be2d1-6826-485f-a531-e2b78711e25a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3e2652a-32ac-4754-8bb5-3a8260021062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    input_text = f\"question: {example['question']} context: {example['context']}\"\n",
    "    target_text = example[\"answers\"][\"text\"][0]\n",
    "    input_enc = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=Max_input_length)\n",
    "    target_enc = tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=Max_output_length)\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88504d94-9c04-4af6-965e-742418624148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "train_enc = train_dataset.map(preprocess, batched=False)\n",
    "val_enc = val_dataset.map(preprocess, batched=False)\n",
    "test_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "058e075b-721c-47f1-9429-c3b7d320aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71900ed4-a318-4d9a-9ccb-36c93a3183f7",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f0bbda5-04be-4603-be6e-887c9fafe9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb0261-4843-4679-8ba7-106988ab1b6e",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a47fac1-457f-4289-b471-5812c15f5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_enc.set_format(type = \"torch\", columns = columns)\n",
    "val_enc.set_format(type = \"torch\", columns = columns)\n",
    "test_enc.set_format(type = \"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d40b6d54-12c3-44f3-88bc-d4463295ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ab5f07-86ee-486e-b702-7a766d6c4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters testing: lr =  1e-05 batch size =  8 weight_decay =  0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='660' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  660/25000 12:28 < 7:41:31, 0.88 it/s, Epoch 0.53/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 46\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     37\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     38\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [EarlyStoppingCallback(early_stopping_patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)]\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     50\u001b[0m display_evaluation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mevaluate(train_enc))\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2241\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2242\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2243\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2244\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2245\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback\n",
    "\n",
    "batch_sizes = [8, 16, 32]\n",
    "learning_rates = [1e-5, 2e-5, 3e-5]\n",
    "weights_decay = [0.1, 0.01, 0.001]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for weight_decay in weights_decay:\n",
    "            try:\n",
    "                del model\n",
    "            except NameError:\n",
    "                pass\n",
    "            \n",
    "            model = T5ForConditionalGeneration.from_pretrained (\"t5-small\")\n",
    "\n",
    "            print(\"Parameters testing: lr = \", learning_rate, \"batch size = \", batch_size, \"weight_decay = \", weight_decay)\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir = \"./results\",\n",
    "                num_train_epochs = 20,\n",
    "                per_device_train_batch_size = batch_size,\n",
    "                per_device_eval_batch_size = 32,\n",
    "                eval_strategy = \"epoch\",\n",
    "                save_strategy = \"epoch\",\n",
    "                learning_rate = learning_rate,\n",
    "                weight_decay = weight_decay,\n",
    "                save_total_limit = 2,\n",
    "                logging_dir = \"./logs\",\n",
    "                logging_steps = 10,\n",
    "            \n",
    "                metric_for_best_model = \"loss\",\n",
    "                load_best_model_at_end = True\n",
    "            )\n",
    "            model.train()\n",
    "                \n",
    "            trainer = Trainer(\n",
    "                model = model,\n",
    "                args = training_args,\n",
    "                train_dataset = train_enc,\n",
    "                eval_dataset = val_enc,\n",
    "                processing_class = tokenizer,\n",
    "                data_collator = DataCollatorForSeq2Seq(tokenizer),\n",
    "                callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    "            )\n",
    "                \n",
    "            trainer.train()\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "            display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af908e-6a12-470a-9809-b5d9d6a69273",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Best parameters:\n",
    "1. learning_rate = 1e-5\n",
    "2. batch_size = 4\n",
    "3. weight_decay = 0.1\n",
    "4. num_train_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "177035f9-acf9-42df-a95b-1b7212d2b6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1500 03:44 < 11:09, 1.68 it/s, Epoch 1.51/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.120884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:4064: UserWarning: mtime may not be reliable on this filesystem, falling back to numerical ordering\n",
      "  warnings.warn(\"mtime may not be reliable on this filesystem, falling back to numerical ordering\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     26\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer)\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     36\u001b[0m display_evaluation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainer\u001b[38;5;241m.\u001b[39mevaluate(train_enc))\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2241\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2242\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2243\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2244\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2245\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 6,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.1,\n",
    "    save_total_limit = 2,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_enc,\n",
    "    eval_dataset = val_enc,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "display_evaluation(\"Training\", trainer.evaluate(train_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e205f24-ef74-4e02-abb9-1dc027543878",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658b7b8-7025-488f-b5a4-a4bfe1c0b86a",
   "metadata": {},
   "source": [
    "## Qualitative Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a255c9-a7d3-457f-a8ac-0c479c96db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166f2a6-b347-40f3-8580-97cebf235dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "\n",
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question} context: {context}\"\n",
    "\n",
    "def extract_sample_parts(sample):\n",
    "    context = sample[\"context\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answers\"][\"text\"][0]\n",
    "    question_with_context = encode_question_and_context(question, context)\n",
    "    return (question_with_context, question, answer)\n",
    "    \n",
    "def generate_response(tokenizer, model, question):\n",
    "    tokenized = tokenizer(question, return_tensors = \"pt\", padding = True, truncation = True,\n",
    "                          max_length = Max_output_length).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**tokenized)\n",
    "\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def generate_answers(tokenizer, model, dataset, use_context = True, limit = None):\n",
    "    if limit is not None:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset:\n",
    "        question_with_context, question, answer = extract_example_parts(example)\n",
    "\n",
    "        if use_context:\n",
    "            inputs.append(question_with_context)\n",
    "\n",
    "        else:\n",
    "            inputs.append(question)\n",
    "\n",
    "        questions.append(question)\n",
    "        \n",
    "        references.append(answer)\n",
    "\n",
    "    outputs = []\n",
    "    for examples in batched(inputs, 128):\n",
    "        responses = generate_response(tokenizer, model, list(examples))\n",
    "\n",
    "        outputs.extend(responses)\n",
    "\n",
    "    return outputs, references, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ffd43-e47b-45a4-bb01-0cfd5ba94d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(\n",
    "    tokenizer, model, test_dataset, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(\n",
    "    tokenizer, model, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4e98d-328a-4c57-8361-28c1f03937f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    "    print(\"Question\", i+1,\":\", question)\n",
    "    print(\"Generated answer:\", answer) \n",
    "    print(\"Reference answer:\", reference)\n",
    "\n",
    "print(\"a. With context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_ctx[i], answers_ctx[i], refs_ctx[i])\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"b. Without context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_noctx[i], answers_noctx[i], refs_noctx[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aba6ba-c428-4e08-8ba2-090d2964a47c",
   "metadata": {},
   "source": [
    "## Model Evaluation using ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2a24a84-ddb1-4c93-aa42-bd68674f8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_score(scores, metric, key):\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "        total += getattr(scores[i][metric], key)\n",
    "    return total / len(scores)\n",
    "\n",
    "def compute_rouge(predictions, references):\n",
    "    metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer = True)\n",
    "\n",
    "    scores = []\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        scores.append(scorer.score(reference, prediction))\n",
    "\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "            results[f\"{metric}_{k}\"] = compute_average_score(scores, metric, k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e43d638b-3207-4535-b70c-da79ac0e9f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE with context: {'rouge1_precision': 0.0, 'rouge1_recall': 0.0, 'rouge1_fmeasure': 0.0, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0, 'rougeL_precision': 0.0, 'rougeL_recall': 0.0, 'rougeL_fmeasure': 0.0}\n",
      "\n",
      "ROUGE without context: {'rouge1_precision': 0.0, 'rouge1_recall': 0.0, 'rouge1_fmeasure': 0.0, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0, 'rougeL_precision': 0.0, 'rougeL_recall': 0.0, 'rougeL_fmeasure': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fec82b-b35a-40c9-8350-48deb0e7231c",
   "metadata": {},
   "source": [
    "## Task 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199002b-3fb9-4bae-8dc4-bea82bfeff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "\n",
    "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88d9ff-8425-4eb8-a91b-a39168eecc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_2, refs_2, questions_2 = generate_answers(\n",
    "    tokenizer_2, model_2, test_dataset, True, 100)\n",
    "answers_n2, refs_n2, questions_n2 = generate_answers(\n",
    "    tokenizer_2, model_2, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be51475-619c-4394-a4ce-d9df5cb007a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a. With context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_2[i], answers_2[i], refs_2[i])\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"b. Without context\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    display_answer_and_references(questions_n2[i], answers_n2[i], refs_n2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e868cf-48f5-489a-8daf-6e2069607b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_2, refs_2))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_n2, refs_n2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
