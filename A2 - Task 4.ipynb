{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a76de3-b775-423c-a194-dad36b7b7117",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, roc_curve\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertweetTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, EarlyStoppingCallback\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/utils/import_utils.py:2045\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2044\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m         module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m   2046\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/utils/import_utils.py:2075\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2073\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2074\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2075\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/utils/import_utils.py:2073\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2072\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2073\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2074\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2075\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/trainer.py:42\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     43\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_hub_utils\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/utils/import_utils.py:2045\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2043\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2044\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m         module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m   2046\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/utils/import_utils.py:2075\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2073\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2074\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2075\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/utils/import_utils.py:2073\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2072\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2073\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2074\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2075\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel, TrainingArguments\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     PushToHubMixin,\n\u001b[32m     41\u001b[39m     flatten_dict,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     logging,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PreTrainedModel' from 'transformers' (/Applications/anaconda3/envs/IFN580/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "import torch\n",
    "from transformers import BertweetTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da123c0-f952-4d1f-a5e9-75ffc9916b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   5000 non-null   int64 \n",
      " 1   text    5000 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/hydrogen.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f56c6-4b8c-46a5-b892-850c4a0043e8",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5723a-3532-4df7-ba0c-e432547f0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83a4b9-2790-444a-972a-dea6891885ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'discover dentists everything dental all in one place'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].iloc[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230984e-6248-4a4e-ac79-b4f505e0b222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>theres hydrogen and helium then lithium beryll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>theres hydrogen and helium then lithium beryll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>got called the square in a brony aerospace mee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>in a hydrogen war ravaged society the nubile y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i am made of flesh lotsa carbon and hydrogen a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  theres hydrogen and helium then lithium beryll...\n",
       "1      0  theres hydrogen and helium then lithium beryll...\n",
       "2      0  got called the square in a brony aerospace mee...\n",
       "3      0  in a hydrogen war ravaged society the nubile y...\n",
       "4      0  i am made of flesh lotsa carbon and hydrogen a..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d066e-d544-456c-9918-31938c6df1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    2736\n",
       "0    2264\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f192c75-e0e1-40c5-ac50-ead43b043a83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m X = df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m      4\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = \u001b[32m0.3\u001b[39m, random_state = random_state)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining set size:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train))\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting set size:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_test))\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "random_state = 10\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = random_state)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Testing set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e72d9-d795-465c-ada8-9bf6963d0064",
   "metadata": {},
   "source": [
    "## Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ff12d-dd5c-4ccc-a77e-4ff4b7e3baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(\"Train dataset:\", train_ds)\n",
    "print(\"Test dataset:\", test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ec351-9923-4cf0-b9f9-e1a690356dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer = BertweetTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a5fbb-d394-48e2-a9c5-186e21549872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding = True)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched = True)\n",
    "test_ds = test_ds.map(tokenize, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69016cbb-410a-4d22-82b6-af694b0c5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80d7a0-1fe1-4473-80b9-072a2455aa46",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d194348-00d4-4a26-8921-cfbf4af1589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels = df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f432e8-bd7a-44b9-bd03-88fb5837fafd",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08157a6-0b76-406b-b937-025b118a9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", pos_label = 1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5de0c-50a6-40ee-8bcb-a193ff57cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 50,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "\n",
    "    metric_for_best_model = \"loss\",\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e959b-613d-4122-b091-a4413b927d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer),\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a18977-80cf-405c-9059-664020d31058",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf5d90-03dc-45b4-88f0-a586a0c9b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "train_results = trainer.evaluate(train_ds)\n",
    "test_results = trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f3b9f-afd3-4f9d-80f5-ba3dfda66913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation(setname, results):\n",
    "    print(f\"{setname} Set Accuracy:\", round(results[\"eval_accuracy\"], 3))\n",
    "    print(f\"{setname} Set Precision:\", round(results[\"eval_precision\"], 3))\n",
    "    print(f\"{setname} Set Recall:\", round(results[\"eval_recall\"], 3))\n",
    "    print(f\"{setname} Set F1 Score:\", round(results[\"eval_f1\"], 3))\n",
    "\n",
    "display_evaluation(\"Training\", train_results)\n",
    "display_evaluation(\"Testing\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0a4ae-1c9f-4467-bed8-b47b9bcc4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_1 = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf829d9-87a2-4d3e-987a-a272976245bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels = df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")\n",
    "\n",
    "model_1.train()\n",
    "\n",
    "trainer_1 = Trainer(\n",
    "    model = model,\n",
    "    args = training_args_1,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4f5b1-ebc5-4579-a8c4-26dc4ba0e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "\n",
    "train_results_1 = trainer_1.evaluate(train_ds)\n",
    "test_results_1 = trainer_1.evaluate(test_ds)\n",
    "\n",
    "display_evaluation(\"Training\", train_results_1)\n",
    "display_evaluation(\"Testing\", test_results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d258a58-ed85-4f7b-b903-56acce79fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_2 = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae88065-77b3-4878-8a50-c715d7157957",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels = df[\"label\"].nunique(),\n",
    "    problem_type=\"single_label_classification\")\n",
    "\n",
    "model_2.train()\n",
    "\n",
    "trainer_2 = Trainer(\n",
    "    model = model,\n",
    "    args = training_args_2,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5623d-1467-463b-92b4-438fcb1917e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.eval()\n",
    "\n",
    "train_results_2 = trainer_2.evaluate(train_ds)\n",
    "test_results_2 = trainer_2.evaluate(test_ds)\n",
    "\n",
    "display_evaluation(\"Training\", train_results_2)\n",
    "display_evaluation(\"Testing\", test_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91181fea-30f0-4f7f-ae6c-e6a8c02bc087",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d15d4-4239-4fb1-ab72-3de4bb6ed770",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 50,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "\n",
    "    metric_for_best_model = \"loss\",\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db180d36-dbf6-485d-b915-7fdaf3115dcc",
   "metadata": {},
   "source": [
    "## Bert-based (uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344e413-c5da-4bde-8008-b2330790fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer_bertbased = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model_bertbased = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58db3f-72d1-45e7-9968-840e42ba1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df_bert = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds_bert = Dataset.from_pandas(train_df_bert)\n",
    "test_ds_bert = Dataset.from_pandas(test_df_bert)\n",
    "\n",
    "train_ds_bert = train_ds_bert.map(tokenize, batched=True)\n",
    "test_ds_bert = test_ds_bert.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f203756-121f-477b-babb-30aa6e25e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_bert = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10\n",
    ")\n",
    "\n",
    "model_bertbased.train()\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model = model_bertbased,\n",
    "    args = training_args_bert,\n",
    "    train_dataset = train_ds_bert,\n",
    "    eval_dataset = test_ds_bert,\n",
    "    data_collator = DataCollatorWithPadding(tokenizer_bertbased),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_bert.train()\n",
    "\n",
    "model_bertbased.eval()\n",
    "\n",
    "train_results_2 = trainer_2.evaluate(train_ds_bert)\n",
    "test_results_2 = trainer_2.evaluate(test_ds_bert)\n",
    "\n",
    "display_evaluation(\"Training\", train_results_2)\n",
    "display_evaluation(\"Testing\", test_results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fc371-1ead-45ed-8b6a-d4f427678f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [8, 16]\n",
    "\n",
    "learning_rate = [1e-5, 2e-5, 3e-5]\n",
    "\n",
    "weight_decay = [0.1, 0.01, 0.001]\n",
    "\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for size in batch_size:\n",
    "        for weight in weight_decay:\n",
    "            try:\n",
    "                del model\n",
    "            except NameError:\n",
    "                pass\n",
    "\n",
    "            training_args_bert = TrainingArguments(\n",
    "                output_dir = \"./results\",\n",
    "                num_train_epochs = 20,\n",
    "                per_device_train_batch_size = size,\n",
    "                per_device_eval_batch_size = 64,\n",
    "                eval_strategy = \"epoch\",\n",
    "                save_strategy = \"epoch\",\n",
    "                learning_rate = lr,\n",
    "                weight_decay = weight,\n",
    "                logging_dir = \"./logs\",\n",
    "                logging_steps = 10,\n",
    "\n",
    "                metric_for_best_model = \"loss\",\n",
    "                load_best_model_at_end = True\n",
    "            )\n",
    "            \n",
    "            model_bertbased.train()\n",
    "            trainer_bert = Trainer(\n",
    "                model = model_bertbased,\n",
    "                args = training_args_bert,\n",
    "                train_dataset = train_ds_bert,\n",
    "                eval_dataset = test_ds_bert,\n",
    "                processing_class = tokenizer_bertbased,\n",
    "                data_collator = DataCollatorWithPadding(tokenizer_bertbased),\n",
    "                compute_metrics = compute_metrics,\n",
    "\n",
    "                callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    "            )\n",
    "\n",
    "            trainer_bert.train()\n",
    "\n",
    "            model_bertbased.eval()\n",
    "            \n",
    "            train_results_2 = trainer_2.evaluate(train_ds_bert)\n",
    "            test_results_2 = trainer_2.evaluate(test_ds_bert)\n",
    "            \n",
    "            display_evaluation(\"Training\", train_results_2)\n",
    "            \n",
    "            display_evaluation(\"Testing\", test_results_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cb05a-3d1c-4a6e-8574-68265dbf5e6c",
   "metadata": {},
   "source": [
    "## Examining Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d769d-553d-40b7-8b9d-23aaf99db9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_matrix(tokenizer, model, text):\n",
    "    tokens  = tokenizer(text, return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens, output_attentions = True)\n",
    "\n",
    "    attentions = torch.stack(pred.attentions).cpu()\n",
    "\n",
    "    attentions = attentions.squeeze(1)\n",
    "\n",
    "    attentions = attentions.mean(dim = 0).mean(dim = 0)\n",
    "\n",
    "    pred_class = pred.logits.cpu().argmax(-1).item()\n",
    "\n",
    "    token_strs = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
    "\n",
    "    return (attentions, pred_class, token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ce6da-5dbe-435a-98fc-c1dc91374eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attentions, tokens, title):\n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.title(title)\n",
    "\n",
    "    sns.heatmap(attentions,\n",
    "    xticklabels = tokens,\n",
    "    yticklabels = tokens,\n",
    "    cmap = 'binary',\n",
    "    cbar = True\n",
    "               )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1cc44d-04e1-4e5d-886e-45d1a6b935e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention_matrix(tokenizer, model, text):\n",
    "    attention, pred_class, tokens = compute_attention_matrix(tokenizer, model, text)\n",
    "    pred_label = \"Relevant\" if pred_class == 1 else \"Not relevant\"\n",
    "    plot_attention(attention, tokens, text + f\"\\nPredicted class: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598a8f6-659d-4585-97af-0e4fc776bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizer, model, df[df[\"label\"] == 0].iloc[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e1128-694e-4b3b-9f29-1e23cd0f2453",
   "metadata": {},
   "source": [
    "## Computing the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1054b44-a385-4993-afa8-8ae20070f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(test_ds)\n",
    "pred_small = small_trainer.predict(small_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6a17d-64e9-4a97-bd87-d64c1a6bc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = torch.nn.functional.softmax(torch.Tensor(pred.predictions)).numpy()\n",
    "pred_probs_small = torch.nn.functional.softmax(torch.Tensor(pred_small.predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba9155-ea76-4bde-a397-788de0d328fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_index_nn = roc_auc_score(y_test, pred_probs[:, 1])\n",
    "roc_index_small = roc_auc_score(y_small_test, pred_probs_small[:, 1])\n",
    "\n",
    "fpr_nn, tpr_nn, bertweet_thresholds_nn = roc_curve(y_test, pred_probs[:, 1])\n",
    "fpr_nn_small, tpr_nn_small, bertweet_thresholds_nn_small = roc_curve(y_small_test, pred_probs_small[:, 1])\n",
    "\n",
    "plt.plot(fpr_nn, tpr_nn, label = \"BERTweet Model: {:.3f}\".format(roc_index_nn), color = 'red', lw = 0.5)\n",
    "plt.plot(fpr_nn_small, tpr_nn_small, label = \"BERTweet Small Model: {:.3f}\".format(roc_index_small), color = 'green', lw = 0.5)\n",
    "plt.plot([0, 1], [0, 1], color = 'navy', lw = 0.5, linestyle = '--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic for positive sentiment\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fdc40-7627-4d77-b0f1-91caaf135cc3",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc019a2-0fe0-4edb-8e1c-d68432a39e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "tfidf = pd.read_csv(\"datasets/tfidf_features.csv\")\n",
    "\n",
    "tfidf.drop(['tweet_id'], axis = 1, inplace = True)\n",
    "\n",
    "y_lr = df['label'].values\n",
    "\n",
    "X_lr = tfidf\n",
    "\n",
    "X_lr = pd.get_dummies(X_lr)\n",
    "\n",
    "random_state = 10\n",
    "test_set_size = 0.3\n",
    "\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr.values, y_lr, test_size = test_set_size,\n",
    "                                                    stratify = y_lr, random_state = random_state)\n",
    "\n",
    "X_train_lr = scaler.fit_transform(X_train_lr, y_train_lr)\n",
    "\n",
    "X_test_lr = scaler.transform(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1606c-b5e2-4c6d-abb5-989952e84d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8d3c6-bc4b-49d5-8e79-07e0b2292c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(random_state = random_state)\n",
    "\n",
    "lr_model.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e031c8-9fa0-4fac-9cda-a8e89fdac031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#training and test accuracy\n",
    "print(\"Train accuracy:\", lr_model.score(X_train_lr, y_train_lr))\n",
    "print(\"Test accuracy:\", lr_model.score(X_test_lr, y_test_lr))\n",
    "\n",
    "# classification report on test data\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_lr)\n",
    "\n",
    "print(classification_report(y_test_lr, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320f8e3-e02f-4f9b-b1d3-95401f40c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_lr = lr_model.predict_proba(X_test_lr)\n",
    "\n",
    "roc_index_lr = roc_auc_score(y_test, pred_probs_lr[:, 1])\n",
    "\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_lr, pred_probs_lr[:, 1])\n",
    "\n",
    "print(\"ROC index on test for lr model:\", roc_index_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5211328-7154-4997-a8c2-b518c1e31fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_nn, tpr_nn, label = \"BERTweet Model: {:.3f}\".format(roc_index_nn), color = 'red', lw = 0.5)\n",
    "plt.plot(fpr_lr, tpr_lr, label = f'Logistic Regression model {roc_index_lr:.3f}', color = 'green', lw = 0.5)\n",
    "plt.plot([0, 1], [0, 1], color = 'navy', lw = 0.5, linestyle = '--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic for positive sentiment\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928928f-aa58-4de1-ba10-40f783b7d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
